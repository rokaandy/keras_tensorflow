{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with keras and tensorflow\n",
    "\n",
    "N.B. You will need to pip install keras and tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson we'll use sklearn's built-in breast cancer dataset. The next cell loads the data and prints the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting our data and initializing a Scaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,\n",
    "                                                    data.target,\n",
    "                                                    random_state=42)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming our data\n",
    "\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_test_s = ss.transform(X_test)\n",
    "\n",
    "X_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34913849, -1.43851335, -0.41172595, -0.39047943, -1.86366229,\n",
       "        -1.26860704, -0.82617052, -0.95286585, -1.72936805, -0.9415409 ,\n",
       "        -0.86971355, -1.35865347, -0.83481506, -0.57230673, -0.74586846,\n",
       "        -0.65398319, -0.52583524, -0.94677147, -0.53781728, -0.63449458,\n",
       "        -0.54268486, -1.65565452, -0.58986401, -0.52555985, -1.51066925,\n",
       "        -0.89149994, -0.75021715, -0.91671059, -0.92508585, -0.80841115]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s[:1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Neural Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing model and layer types\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Import optimizer\n",
    "\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Constructing and compiling our model\n",
    "#Create Model\n",
    "model = Sequential()\n",
    "\n",
    "#Create Input\n",
    "inputs = X_train.shape[1]\n",
    "\n",
    "\n",
    "hiddens = inputs\n",
    "\n",
    "#Add hidden nodes\n",
    "model.add(Dense(hiddens, input_dim=inputs, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "adam = Adam()\n",
    "\n",
    "#Compile ** Re-Run compile statement if we are changing epochs number in next cell **\n",
    "model.compile(optimizer=adam, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/99\n",
      "426/426 [==============================] - 0s 567us/step - loss: 1.8395 - val_loss: 1.1748\n",
      "Epoch 2/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.9343 - val_loss: 0.6084\n",
      "Epoch 3/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.4668 - val_loss: 0.3390\n",
      "Epoch 4/99\n",
      "426/426 [==============================] - 0s 59us/step - loss: 0.2823 - val_loss: 0.2277\n",
      "Epoch 5/99\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.2189 - val_loss: 0.1819\n",
      "Epoch 6/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.1867 - val_loss: 0.1590\n",
      "Epoch 7/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.1660 - val_loss: 0.1476\n",
      "Epoch 8/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.1462 - val_loss: 0.1314\n",
      "Epoch 9/99\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.1345 - val_loss: 0.1264\n",
      "Epoch 10/99\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.1225 - val_loss: 0.1161\n",
      "Epoch 11/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.1128 - val_loss: 0.1115\n",
      "Epoch 12/99\n",
      "426/426 [==============================] - 0s 58us/step - loss: 0.1033 - val_loss: 0.1103\n",
      "Epoch 13/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0968 - val_loss: 0.1033\n",
      "Epoch 14/99\n",
      "426/426 [==============================] - 0s 59us/step - loss: 0.0904 - val_loss: 0.1009\n",
      "Epoch 15/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0849 - val_loss: 0.0997\n",
      "Epoch 16/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0804 - val_loss: 0.0967\n",
      "Epoch 17/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0765 - val_loss: 0.0925\n",
      "Epoch 18/99\n",
      "426/426 [==============================] - 0s 63us/step - loss: 0.0729 - val_loss: 0.0939\n",
      "Epoch 19/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0694 - val_loss: 0.0907\n",
      "Epoch 20/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0667 - val_loss: 0.0883\n",
      "Epoch 21/99\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0643 - val_loss: 0.0882\n",
      "Epoch 22/99\n",
      "426/426 [==============================] - 0s 59us/step - loss: 0.0622 - val_loss: 0.0862\n",
      "Epoch 23/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0601 - val_loss: 0.0851\n",
      "Epoch 24/99\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0583 - val_loss: 0.0829\n",
      "Epoch 25/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0565 - val_loss: 0.0823\n",
      "Epoch 26/99\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0560 - val_loss: 0.0816\n",
      "Epoch 27/99\n",
      "426/426 [==============================] - 0s 109us/step - loss: 0.0540 - val_loss: 0.0799\n",
      "Epoch 28/99\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0530 - val_loss: 0.0791\n",
      "Epoch 29/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0520 - val_loss: 0.0811\n",
      "Epoch 30/99\n",
      "426/426 [==============================] - 0s 66us/step - loss: 0.0509 - val_loss: 0.0771\n",
      "Epoch 31/99\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0497 - val_loss: 0.0766\n",
      "Epoch 32/99\n",
      "426/426 [==============================] - 0s 58us/step - loss: 0.0486 - val_loss: 0.0759\n",
      "Epoch 33/99\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0479 - val_loss: 0.0743\n",
      "Epoch 34/99\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0471 - val_loss: 0.0731\n",
      "Epoch 35/99\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0461 - val_loss: 0.0725\n",
      "Epoch 36/99\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0453 - val_loss: 0.0715\n",
      "Epoch 37/99\n",
      "426/426 [==============================] - 0s 63us/step - loss: 0.0447 - val_loss: 0.0726\n",
      "Epoch 38/99\n",
      "426/426 [==============================] - 0s 58us/step - loss: 0.0442 - val_loss: 0.0708\n",
      "Epoch 39/99\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0434 - val_loss: 0.0716\n",
      "Epoch 40/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0427 - val_loss: 0.0698\n",
      "Epoch 41/99\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0418 - val_loss: 0.0687\n",
      "Epoch 42/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0414 - val_loss: 0.0686\n",
      "Epoch 43/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0407 - val_loss: 0.0679\n",
      "Epoch 44/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0402 - val_loss: 0.0678\n",
      "Epoch 45/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0395 - val_loss: 0.0675\n",
      "Epoch 46/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0392 - val_loss: 0.0680\n",
      "Epoch 47/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0382 - val_loss: 0.0668\n",
      "Epoch 48/99\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0377 - val_loss: 0.0674\n",
      "Epoch 49/99\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0373 - val_loss: 0.0656\n",
      "Epoch 50/99\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0367 - val_loss: 0.0661\n",
      "Epoch 51/99\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0363 - val_loss: 0.0657\n",
      "Epoch 52/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0356 - val_loss: 0.0652\n",
      "Epoch 53/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0350 - val_loss: 0.0652\n",
      "Epoch 54/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0344 - val_loss: 0.0656\n",
      "Epoch 55/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0340 - val_loss: 0.0638\n",
      "Epoch 56/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0336 - val_loss: 0.0639\n",
      "Epoch 57/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0334 - val_loss: 0.0643\n",
      "Epoch 58/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0329 - val_loss: 0.0637\n",
      "Epoch 59/99\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0325 - val_loss: 0.0628\n",
      "Epoch 60/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0322 - val_loss: 0.0637\n",
      "Epoch 61/99\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0317 - val_loss: 0.0641\n",
      "Epoch 62/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0315 - val_loss: 0.0615\n",
      "Epoch 63/99\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0314 - val_loss: 0.0636\n",
      "Epoch 64/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0308 - val_loss: 0.0612\n",
      "Epoch 65/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0303 - val_loss: 0.0619\n",
      "Epoch 66/99\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0299 - val_loss: 0.0607\n",
      "Epoch 67/99\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0295 - val_loss: 0.0613\n",
      "Epoch 68/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0291 - val_loss: 0.0610\n",
      "Epoch 69/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0289 - val_loss: 0.0608\n",
      "Epoch 70/99\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0284 - val_loss: 0.0608\n",
      "Epoch 71/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0280 - val_loss: 0.0599\n",
      "Epoch 72/99\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0277 - val_loss: 0.0607\n",
      "Epoch 73/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0278 - val_loss: 0.0588\n",
      "Epoch 74/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0271 - val_loss: 0.0600\n",
      "Epoch 75/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0271 - val_loss: 0.0601\n",
      "Epoch 76/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0264 - val_loss: 0.0585\n",
      "Epoch 77/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0261 - val_loss: 0.0606\n",
      "Epoch 78/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0260 - val_loss: 0.0578\n",
      "Epoch 79/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0256 - val_loss: 0.0580\n",
      "Epoch 80/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0253 - val_loss: 0.0585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0251 - val_loss: 0.0579\n",
      "Epoch 82/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0248 - val_loss: 0.0581\n",
      "Epoch 83/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0246 - val_loss: 0.0573\n",
      "Epoch 84/99\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0243 - val_loss: 0.0572\n",
      "Epoch 85/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0241 - val_loss: 0.0582\n",
      "Epoch 86/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0239 - val_loss: 0.0564\n",
      "Epoch 87/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0233 - val_loss: 0.0565\n",
      "Epoch 88/99\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0231 - val_loss: 0.0566\n",
      "Epoch 89/99\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0227 - val_loss: 0.0555\n",
      "Epoch 90/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0225 - val_loss: 0.0560\n",
      "Epoch 91/99\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0223 - val_loss: 0.0556\n",
      "Epoch 92/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0222 - val_loss: 0.0564\n",
      "Epoch 93/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0219 - val_loss: 0.0543\n",
      "Epoch 94/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0215 - val_loss: 0.0550\n",
      "Epoch 95/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0213 - val_loss: 0.0551\n",
      "Epoch 96/99\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0212 - val_loss: 0.0560\n",
      "Epoch 97/99\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0209 - val_loss: 0.0541\n",
      "Epoch 98/99\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0209 - val_loss: 0.0564\n",
      "Epoch 99/99\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0205 - val_loss: 0.0545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a299c9320>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model\n",
    "\n",
    "model.fit(X_train_s, y_train, validation_data=(X_test_s, y_test), epochs=99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/99\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0204 - val_loss: 0.0553\n",
      "Epoch 2/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0201 - val_loss: 0.0539\n",
      "Epoch 3/99\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0199 - val_loss: 0.0547\n",
      "Epoch 4/99\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.0197 - val_loss: 0.0548\n",
      "Epoch 5/99\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0195 - val_loss: 0.0520\n",
      "Epoch 6/99\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0198 - val_loss: 0.0539\n",
      "Epoch 7/99\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0192 - val_loss: 0.0530\n",
      "Epoch 8/99\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.0190 - val_loss: 0.0547\n",
      "Epoch 9/99\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.0187 - val_loss: 0.0531\n",
      "Epoch 10/99\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0189 - val_loss: 0.0535\n",
      "Epoch 11/99\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.0183 - val_loss: 0.0534\n",
      "Epoch 12/99\n",
      "426/426 [==============================] - 0s 58us/step - loss: 0.0183 - val_loss: 0.0524\n",
      "Epoch 13/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0182 - val_loss: 0.0537\n",
      "Epoch 14/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0183 - val_loss: 0.0549\n",
      "Epoch 15/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0179 - val_loss: 0.0534\n",
      "Epoch 16/99\n",
      "426/426 [==============================] - 0s 59us/step - loss: 0.0175 - val_loss: 0.0525\n",
      "Epoch 17/99\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0177 - val_loss: 0.0544\n",
      "Epoch 18/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0178 - val_loss: 0.0514\n",
      "Epoch 19/99\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.0175 - val_loss: 0.0521\n",
      "Epoch 20/99\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0169 - val_loss: 0.0519\n",
      "Epoch 21/99\n",
      "426/426 [==============================] - 0s 58us/step - loss: 0.0168 - val_loss: 0.0537\n",
      "Epoch 22/99\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0170 - val_loss: 0.0519\n",
      "Epoch 23/99\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.0176 - val_loss: 0.0523\n",
      "Epoch 24/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0165 - val_loss: 0.0524\n",
      "Epoch 25/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0165 - val_loss: 0.0532\n",
      "Epoch 26/99\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.0163 - val_loss: 0.0509\n",
      "Epoch 27/99\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0161 - val_loss: 0.0524\n",
      "Epoch 28/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0160 - val_loss: 0.0519\n",
      "Epoch 29/99\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0159 - val_loss: 0.0510\n",
      "Epoch 30/99\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0158 - val_loss: 0.0503\n",
      "Epoch 31/99\n",
      "426/426 [==============================] - 0s 38us/step - loss: 0.0157 - val_loss: 0.0528\n",
      "Epoch 32/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0516\n",
      "Epoch 33/99\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0517\n",
      "Epoch 34/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0157 - val_loss: 0.0503\n",
      "Epoch 35/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0153 - val_loss: 0.0511\n",
      "Epoch 36/99\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0152 - val_loss: 0.0509\n",
      "Epoch 37/99\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0147 - val_loss: 0.0505\n",
      "Epoch 38/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0147 - val_loss: 0.0519\n",
      "Epoch 39/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0148 - val_loss: 0.0501\n",
      "Epoch 40/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0144 - val_loss: 0.0503\n",
      "Epoch 41/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0145 - val_loss: 0.0501\n",
      "Epoch 42/99\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0147 - val_loss: 0.0487\n",
      "Epoch 43/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0146 - val_loss: 0.0514\n",
      "Epoch 44/99\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0141 - val_loss: 0.0491\n",
      "Epoch 45/99\n",
      "426/426 [==============================] - 0s 63us/step - loss: 0.0144 - val_loss: 0.0494\n",
      "Epoch 46/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0140 - val_loss: 0.0501\n",
      "Epoch 47/99\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0137 - val_loss: 0.0494\n",
      "Epoch 48/99\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0510\n",
      "Epoch 49/99\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.0136 - val_loss: 0.0509\n",
      "Epoch 50/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0512\n",
      "Epoch 51/99\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0136 - val_loss: 0.0485\n",
      "Epoch 52/99\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0136 - val_loss: 0.0502\n",
      "Epoch 53/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0137 - val_loss: 0.0476\n",
      "Epoch 54/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0520\n",
      "Epoch 55/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0131 - val_loss: 0.0504\n",
      "Epoch 56/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0131 - val_loss: 0.0505\n",
      "Epoch 57/99\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.0128 - val_loss: 0.0504\n",
      "Epoch 58/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0127 - val_loss: 0.0498\n",
      "Epoch 59/99\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0126 - val_loss: 0.0504\n",
      "Epoch 60/99\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0126 - val_loss: 0.0507\n",
      "Epoch 61/99\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0124 - val_loss: 0.0491\n",
      "Epoch 62/99\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0124 - val_loss: 0.0505\n",
      "Epoch 63/99\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0121 - val_loss: 0.0510\n",
      "Epoch 64/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0125 - val_loss: 0.0512\n",
      "Epoch 65/99\n",
      "426/426 [==============================] - 0s 36us/step - loss: 0.0123 - val_loss: 0.0497\n",
      "Epoch 66/99\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0122 - val_loss: 0.0494\n",
      "Epoch 67/99\n",
      "426/426 [==============================] - 0s 41us/step - loss: 0.0122 - val_loss: 0.0481\n",
      "Epoch 68/99\n",
      "426/426 [==============================] - 0s 44us/step - loss: 0.0121 - val_loss: 0.0515\n",
      "Epoch 69/99\n",
      "426/426 [==============================] - 0s 40us/step - loss: 0.0122 - val_loss: 0.0478\n",
      "Epoch 70/99\n",
      "426/426 [==============================] - 0s 39us/step - loss: 0.0117 - val_loss: 0.0491\n",
      "Epoch 71/99\n",
      "426/426 [==============================] - 0s 37us/step - loss: 0.0116 - val_loss: 0.0504\n",
      "Epoch 72/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0117 - val_loss: 0.0500\n",
      "Epoch 73/99\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0120 - val_loss: 0.0494\n",
      "Epoch 74/99\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0116 - val_loss: 0.0488\n",
      "Epoch 75/99\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0116 - val_loss: 0.0489\n",
      "Epoch 76/99\n",
      "426/426 [==============================] - 0s 45us/step - loss: 0.0113 - val_loss: 0.0488\n",
      "Epoch 77/99\n",
      "426/426 [==============================] - 0s 42us/step - loss: 0.0110 - val_loss: 0.0479\n",
      "Epoch 78/99\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0110 - val_loss: 0.0494\n",
      "Epoch 79/99\n",
      "426/426 [==============================] - 0s 43us/step - loss: 0.0110 - val_loss: 0.0493\n",
      "Epoch 80/99\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0109 - val_loss: 0.0484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/99\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0107 - val_loss: 0.0497\n",
      "Epoch 82/99\n",
      "426/426 [==============================] - 0s 62us/step - loss: 0.0107 - val_loss: 0.0486\n",
      "Epoch 83/99\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0110 - val_loss: 0.0487\n",
      "Epoch 84/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0110 - val_loss: 0.0485\n",
      "Epoch 85/99\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0121 - val_loss: 0.0518\n",
      "Epoch 86/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0109 - val_loss: 0.0500\n",
      "Epoch 87/99\n",
      "426/426 [==============================] - 0s 46us/step - loss: 0.0111 - val_loss: 0.0498\n",
      "Epoch 88/99\n",
      "426/426 [==============================] - 0s 54us/step - loss: 0.0105 - val_loss: 0.0495\n",
      "Epoch 89/99\n",
      "426/426 [==============================] - 0s 50us/step - loss: 0.0105 - val_loss: 0.0491\n",
      "Epoch 90/99\n",
      "426/426 [==============================] - 0s 47us/step - loss: 0.0103 - val_loss: 0.0495\n",
      "Epoch 91/99\n",
      "426/426 [==============================] - 0s 49us/step - loss: 0.0102 - val_loss: 0.0503\n",
      "Epoch 92/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0102 - val_loss: 0.0486\n",
      "Epoch 93/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0100 - val_loss: 0.0490\n",
      "Epoch 94/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0101 - val_loss: 0.0505\n",
      "Epoch 95/99\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0099 - val_loss: 0.0483\n",
      "Epoch 96/99\n",
      "426/426 [==============================] - 0s 53us/step - loss: 0.0098 - val_loss: 0.0499\n",
      "Epoch 97/99\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0098 - val_loss: 0.0505\n",
      "Epoch 98/99\n",
      "426/426 [==============================] - 0s 59us/step - loss: 0.0097 - val_loss: 0.0488\n",
      "Epoch 99/99\n",
      "426/426 [==============================] - 0s 48us/step - loss: 0.0093 - val_loss: 0.0490\n"
     ]
    }
   ],
   "source": [
    "# Storing that fit as a history log\n",
    "\n",
    "history_log = model.fit(X_train_s, y_train, validation_data=(X_test_s, y_test), epochs=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting our losses\n",
    "\n",
    "train_loss = history_log.history['loss']\n",
    "test_loss = history_log.history['val_loss']\n",
    "\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Test Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow as a graph constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the graph\n",
    "\n",
    "a = tf.Variable(4)\n",
    "b = tf.Variable(5)\n",
    "\n",
    "c = a + b\n",
    "d = a + c * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = sess.run(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "# Printing the output\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-44-d26d6f2c7d0c>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:514: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, 30))\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "hid = tf.layers.dense(X, 30, activation=tf.nn.relu)\n",
    "y_hat = tf.layers.dense(hid, 1, activation=tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "training_run = optimizer.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986013986013986"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        sess.run(training_run, feed_dict={X: X_train_s,\n",
    "                                          y: y_train.reshape(-1, 1)})\n",
    "        \n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test_s})\n",
    "\n",
    "classes = (pred > 0.5).astype(int)\n",
    "\n",
    "metrics.accuracy_score(y_test.reshape(-1, 1), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
